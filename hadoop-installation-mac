Hadoop setup on Macbook
Download
Download: https://archive.apache.org/dist/hadoop/common/hadoop-2.8.0/
Jdk download: http://www.oracle.com/technetwork/java/javase/downloads/index.html

Install hadoop
tar -zxvf hadoop-2.8.0.tar.gz

run vi /etc/profile to edit your environment with below
JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
HADOOP_HOME=/Users/evenchen/Documents/Hadoop/hadoop-2.8.0
PATH=$JAVA_HOME/bin::$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH:
export JAVA_HOME
export CLASSPATH
export PATH

after that, please run source /etc/profile to make it effective
and then please run hadoop version command to verify it,below is my result after running hadoop version

Evens-MBP:~ evenchen$ hadoop version
Hadoop 2.8.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 91f2b7a13d1e97be65db92ddabc627cc29ac0009
Compiled by jdu on 2017-03-17T04:12Z
Compiled with protoc 2.5.0
From source with checksum 60125541c2b3e266cbf3becc5bda666
This command was run using /Users/evenchen/Documents/Hadoop/hadoop-2.8.0/share/hadoop/common/hadoop-common-2.8.0.jar

Hadoop Configuration

Go to $hadoop_dir/etc/hadoop, and edit hadoop-env.sh to setup java_home
# The java implementation to use, do not use $java_home, as hadoop cannot support it well.
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home

Go to $hadoop_dir/etc/hadoop, and edit core-site.xml to setup hadoop file system
<configuration>
	<!--config temporary directory-->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/Users/evenchen/Documents/Hadoop/hadoop-2.8.0/data</value>
    </property>
    <!--config file system-->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

Go to $hadoop_dir/etc/hadoop, and edit hdfs-site.xml as below as we have only 1 machine.
<configuration>
	<property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

Go to $hadoop_dir/etc/hadoop, please create mapred-site.xml with below contents if you don't have(normallly mapred-site.xml is not exist)
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
	<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>


Go to $hadoop_dir/etc/hadoop, and edit yarn-site.xml as below. 
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>localhost:9000</value>
    </property>
</configuration>

Then run hadoop namenode -format to start hadoop
and then run start-dfs.sh to start hdfs, please note you may need to input password 3 times; 
and if you got Connection Refused issue, please folllow below steps to resolve it:
1. Go to System Preferences
2. Click Sharing icon to to open it
3. Checked Remote Login and select your account in the Allow Access For panel. 

Please run start-yarn.sh to start yarn

please run jps to verify it, if no issue, then it means hadoop platform is started successfully.
then you can open http://localhost:50070/ to verify it. 


please reference below below guide:
https://www.cnblogs.com/fishbay/archive/2017/07/24/7229502.html


